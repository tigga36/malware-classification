{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Notebook for Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_contents(file_loc, first_n=-1):\n",
    "    package = []\n",
    "    dataDir = file_loc\n",
    "    files = sorted(glob.glob(dataDir+\"*.json\"))\n",
    "    for f in files[:first_n]:\n",
    "        read_file = open(f)\n",
    "        package.append(json.load(read_file)['strings'])\n",
    "    \n",
    "    return package\n",
    "\n",
    "def train_and_save_d2v_model_def_par(doc_set, name_of_model='my_doc2vec_model', vec_size=400):\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(doc_set)]\n",
    "    model = Doc2Vec(documents, vector_size=vec_size, window=2, min_count=1, workers=4)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_save_d2v_model(doc_set, name_of_model='my_doc2vec_model', vec_size=400):\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(doc_set)]\n",
    "    model = Doc2Vec(documents, vector_size=vec_size, window=1, min_count=2, workers=2, alpha=0.075, epochs=20)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def construct_feature_vector_set(vec_set, model, is_positive_set):\n",
    "    X_raw = []\n",
    "    for vec in vec_set:\n",
    "        X_raw.append(model.infer_vector(vec))\n",
    "    \n",
    "    if(is_positive_set):\n",
    "        y = np.array([1.]*len(vec_set))\n",
    "    else:\n",
    "        y = np.array([0.]*len(vec_set))\n",
    "\n",
    "    return np.array(X_raw), y\n",
    "\n",
    "def train_svm_class(train_data, train_label):\n",
    "    clf = SVC(C=1, gamma=0.01)\n",
    "    clf.fit(train_data, train_label)\n",
    "    return clf\n",
    "\n",
    "def train_svm_class_def_par(train_data, train_label):\n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    clf.fit(train_data, train_label)\n",
    "    return clf\n",
    "\n",
    "def print_roc(clf, X, y):\n",
    "    y_score = clf.decision_function(X)\n",
    "    fpr, tpr, _ = roc_curve(y, clf.predict_proba(X)[:,1], pos_label=clf.classes_[1])\n",
    "    print('AUC: ', sklearn.metrics.auc(fpr, tpr))\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "    \n",
    "def full_course(direc_name, model_name):\n",
    "    pos_data = read_json_contents(direc_name + 'FFRI_strings/ffri_2013_strings/') + read_json_contents(direc_name + 'FFRI_strings/ffri_2014_strings/')\n",
    "    fal_data = read_json_contents(direc_name + 'Cnet_strings/CnetA/')\n",
    "    pos_test_data_15 = read_json_contents(direc_name + 'FFRI_strings/ffri_2015_strings/')\n",
    "    pos_test_data_16 = read_json_contents(direc_name + 'FFRI_strings/ffri_2016_strings/')\n",
    "    pos_test_data_17 = read_json_contents(direc_name + 'FFRI_strings/ffri_2017_strings/')\n",
    "    fal_test_data = read_json_contents(direc_name + 'Cnet_strings/CnetB/')\n",
    "    \n",
    "    model_train = train_and_save_d2v_model(pos_data+fal_data, model_name)\n",
    "    \n",
    "    X_pos, y_pos = construct_feature_vector_set(pos_data, model_train, True)\n",
    "    X_fal, y_fal = construct_feature_vector_set(fal_data, model_train, False)\n",
    "    X_test_pos_15, y_test_pos_15 = construct_feature_vector_set(pos_test_data_15, model_train, True)\n",
    "    X_test_pos_16, y_test_pos_16 = construct_feature_vector_set(pos_test_data_16, model_train, True)\n",
    "    X_test_pos_17, y_test_pos_17 = construct_feature_vector_set(pos_test_data_17, model_train, True)\n",
    "    X_test_fal, y_test_fal = construct_feature_vector_set(fal_test_data, model_train, False)\n",
    "\n",
    "    X_train = np.append(X_pos,X_fal, axis=0)\n",
    "    y_train = np.append(y_pos,y_fal)\n",
    "    X_test_15 = np.append(X_test_pos_15,X_test_fal, axis=0)\n",
    "    y_test_15 = np.append(y_test_pos_15,y_test_fal)\n",
    "    X_test_16 = np.append(X_test_pos_16,X_test_fal, axis=0)\n",
    "    y_test_16 = np.append(y_test_pos_16,y_test_fal)\n",
    "    X_test_17 = np.append(X_test_pos_17,X_test_fal, axis=0)\n",
    "    y_test_17 = np.append(y_test_pos_17,y_test_fal)\n",
    "\n",
    "    clf_try = train_svm_class(X_train, y_train)\n",
    "    \n",
    "    print('FFRI 2015:')\n",
    "    y_result = clf_try.predict(X_test_15)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test_15, y_result)\n",
    "    prf = precision_recall_fscore_support(y_test_15, y_result, average='macro')\n",
    "    print('acc: ', acc)\n",
    "    print('prf:', prf)\n",
    "    \n",
    "    print('FFRI 2016:')\n",
    "    y_result = clf_try.predict(X_test_16)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test_16, y_result)\n",
    "    prf = precision_recall_fscore_support(y_test_16, y_result, average='macro')\n",
    "    print('acc: ', acc)\n",
    "    print('prf:', prf)\n",
    "    \n",
    "    print('FFRI 2017:')\n",
    "    y_result = clf_try.predict(X_test_17)\n",
    "    acc = sklearn.metrics.accuracy_score(y_test_17, y_result)\n",
    "    prf = precision_recall_fscore_support(y_test_17, y_result, average='macro')\n",
    "    print('acc: ', acc)\n",
    "    print('prf:', prf)\n",
    "    \n",
    "def full_course_def_par(direc_name, model_name):\n",
    "    pos_data = read_json_contents(direc_name + 'FFRI_strings/ffri_2013_strings/') + read_json_contents(direc_name + 'FFRI_strings/ffri_2014_strings/')\n",
    "    fal_data = read_json_contents(direc_name + 'Cnet_strings/CnetA/')\n",
    "    pos_test_data = read_json_contents(direc_name + 'FFRI_strings/ffri_2015_strings/') + read_json_contents(direc_name + 'FFRI_strings/ffri_2016_strings/') + read_json_contents(direc_name + 'FFRI_strings/ffri_2017_strings/')\n",
    "    fal_test_data = read_json_contents(direc_name + 'Cnet_strings/CnetB/')\n",
    "    \n",
    "    model_train = train_and_save_d2v_model_def_par(pos_data+fal_data, model_name)\n",
    "    \n",
    "    X_pos, y_pos = construct_feature_vector_set(pos_data, model_train, True)\n",
    "    X_fal, y_fal = construct_feature_vector_set(fal_data, model_train, False)\n",
    "    X_test_pos, y_test_pos = construct_feature_vector_set(pos_test_data, model_train, True)\n",
    "    X_test_fal, y_test_fal = construct_feature_vector_set(fal_test_data, model_train, False)\n",
    "    \n",
    "    X_train = np.append(X_pos,X_fal, axis=0)\n",
    "    y_train = np.append(y_pos,y_fal)\n",
    "    X_test = np.append(X_test_pos,X_test_fal, axis=0)\n",
    "    y_test = np.append(y_test_pos,y_test_fal)\n",
    "\n",
    "    clf_try = train_svm_class_def_par(X_train, y_train)\n",
    "    \n",
    "    y_result = clf_try.predict(X_test)\n",
    "    \n",
    "    acc = sklearn.metrics.accuracy_score(y_test, y_result)\n",
    "    print('acc: ', acc)\n",
    "    \n",
    "    return precision_recall_fscore_support(y_test, y_result, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-666a660fdbe5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfal_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtesto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-666a660fdbe5>\u001b[0m in \u001b[0;36mtesto\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtesto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdirec_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/strings/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpos_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_json_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FFRI_strings/ffri_2013_strings/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mread_json_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FFRI_strings/ffri_2014_strings/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_json_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Cnet_strings/CnetA/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpos_test_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_json_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FFRI_strings/ffri_2015_strings/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mread_json_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FFRI_strings/ffri_2016_strings/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mread_json_contents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FFRI_strings/ffri_2017_strings/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-66d402d71c62>\u001b[0m in \u001b[0;36mread_json_contents\u001b[0;34m(file_loc, first_n)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfirst_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mread_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mpackage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'strings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.2/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.2/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def testo():\n",
    "    direc_name='data/strings/'\n",
    "    pos_data = read_json_contents(direc_name + 'FFRI_strings/ffri_2013_strings/') + read_json_contents(direc_name + 'FFRI_strings/ffri_2014_strings/')\n",
    "    fal_data = read_json_contents(direc_name + 'Cnet_strings/CnetA/')\n",
    "    pos_test_data = read_json_contents(direc_name + 'FFRI_strings/ffri_2015_strings/') + read_json_contents(direc_name + 'FFRI_strings/ffri_2016_strings/') + read_json_contents(direc_name + 'FFRI_strings/ffri_2017_strings/')\n",
    "    fal_test_data = read_json_contents(direc_name + 'Cnet_strings/CnetB/')\n",
    "    \n",
    "    print(len(pos_data))\n",
    "    print(len(fal_data))\n",
    "    print(len(pos_test_data))\n",
    "    print(len(fal_test_data))\n",
    "\n",
    "testo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Types of Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings (Initial Version with initial default parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.7511396756595172\n"
     ]
    }
   ],
   "source": [
    "initial_strings_with_def_par = full_course_def_par('data/strings/', 'initial_strings_model_with_default_parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7638022895527972, 0.7897201686435518, 0.7476883265877674, None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_strings_with_def_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFRI 2015:\n",
      "acc:  0.8315403422982885\n",
      "prf: (0.784879053474236, 0.7266688040298479, 0.7476340527641172, None)\n",
      "FFRI 2016:\n",
      "acc:  0.7593787472163536\n",
      "prf: (0.7937362072632743, 0.7486135936826959, 0.7462815352011963, None)\n",
      "FFRI 2017:\n",
      "acc:  0.8105147864184009\n",
      "prf: (0.8254457027679345, 0.7811952195016718, 0.7914033196427055, None)\n"
     ]
    }
   ],
   "source": [
    "initial_strings = full_course('data/strings/', 'v1_paper_parameters')\n",
    "initial_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFRI 2015:\n",
      "acc:  0.8079869600651997\n",
      "prf: (0.7411045147230754, 0.7130000080205069, 0.7245885603777531, None)\n",
      "FFRI 2016:\n",
      "acc:  0.724775880774282\n",
      "prf: (0.7525730405607225, 0.7139110534316644, 0.7098428010077257, None)\n",
      "FFRI 2017:\n",
      "acc:  0.780362090071516\n",
      "prf: (0.7862166035600944, 0.7517321453996333, 0.7596725918727539, None)\n"
     ]
    }
   ],
   "source": [
    "initial_strings = full_course('data/0728/strings_v2/', 'v2_paper_parameters')\n",
    "initial_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFRI 2015:\n",
      "acc:  0.8272208638956805\n",
      "prf: (0.7775580639508922, 0.7207650815471551, 0.7411266791426588, None)\n",
      "FFRI 2016:\n",
      "acc:  0.7541826072060754\n",
      "prf: (0.7886333964682343, 0.7432681540345546, 0.7405243469745554, None)\n",
      "FFRI 2017:\n",
      "acc:  0.8112879324785774\n",
      "prf: (0.8247215009229012, 0.7828329975191457, 0.7928198967417412, None)\n"
     ]
    }
   ],
   "source": [
    "initial_strings = full_course('data/0728/strings_v3/', 'v3_paper_parameters')\n",
    "initial_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFRI 2015:\n",
      "acc:  0.8431947840260798\n",
      "prf: (0.8176053541593638, 0.7267112623634046, 0.7552908429491527, None)\n",
      "FFRI 2016:\n",
      "acc:  0.7305430251812939\n",
      "prf: (0.7883285747491506, 0.716557105178113, 0.7075965212158339, None)\n",
      "FFRI 2017:\n",
      "acc:  0.7892532697635462\n",
      "prf: (0.8231148377052591, 0.7492947902060187, 0.759614367573936, None)\n"
     ]
    }
   ],
   "source": [
    "initial_strings = full_course('data/0728/strings_v4/', 'v4_paper_parameters')\n",
    "initial_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFRI 2015:\n",
      "acc:  0.7687856560717197\n",
      "prf: (0.6782373167250552, 0.5889310566938701, 0.5969982858471721, None)\n",
      "FFRI 2016:\n",
      "acc:  0.6209672814480671\n",
      "prf: (0.6928519053871027, 0.6010030455854193, 0.5589080221645246, None)\n",
      "FFRI 2017:\n",
      "acc:  0.6773403775529927\n",
      "prf: (0.7135473262796683, 0.6136715435228131, 0.5969714410273673, None)\n"
     ]
    }
   ],
   "source": [
    "initial_strings = full_course('data/strings_v5/new_', 'v5_paper_parameters')\n",
    "initial_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***After these runs are done, you should consider removing the standardscalar() from the SVM pipeline. Based on the docs, this appears to be normalizing features column-wise throughout the vector. That probably means its normalizing it in 400 different ways. I have a gut feeling that may be messing with the way these are detected. On the other hand, this isn't computer vision, but a statistic. Therefore, it may not really have an effect, and actually might be helping in the long run.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f_score(precision, recall):\n",
    "    return 2*recall*precision/(recall+precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.781445937300064\n",
      "0.7993621013133207\n",
      "0.7730575307045895\n"
     ]
    }
   ],
   "source": [
    "print(calc_f_score(0.788,0.775))\n",
    "print(calc_f_score(0.810,0.789))\n",
    "print(calc_f_score(0.792,0.755))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (nlp-malware)",
   "language": "python",
   "name": "nlp-malware"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
